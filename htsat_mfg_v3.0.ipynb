{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "## Tutorial on training a HTS-AT model for audio classification on the ESC-50 Dataset\n",
    "\n",
    "Referece: \n",
    "\n",
    "[HTS-AT: A Hierarchical Token-Semantic Audio Transformer for Sound Classification and Detection, ICASSP 2022](https://arxiv.org/abs/2202.00874)\n",
    "\n",
    "Following the HTS-AT's paper, in this tutorial, we would show how to use the HST-AT in the training of the ESC-50 Dataset.\n",
    "\n",
    "The [ESC-50 dataset](https://github.com/karolpiczak/ESC-50) is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification. The dataset consists of 5-second-long recordings organized into 50 semantical classes (with 40 examples per class) loosely arranged into 5 major categories\n",
    "\n",
    "Before running this tutorial, please make sure that you install the below packages by following steps:\n",
    "\n",
    "1. download [the codebase](https://github.com/RetroCirce/HTS-Audio-Transformer), and put this tutorial notebook inside the codebase folder.\n",
    "\n",
    "2. In the github code folder:\n",
    "\n",
    "    > pip install -r requirements.txt\n",
    "\n",
    "3. We do not include the installation of PyTorch in the requirment, since different machines require different vereions of CUDA and Toolkits. So make sure you install the PyTorch from [the official guidance](https://pytorch.org/).\n",
    "\n",
    "4. Install the 'SOX' and the 'ffmpeg', we recommend that you run this code in Linux inside the Conda environment. In that, you can install them by:\n",
    "\n",
    "    > sudo apt install sox\n",
    "    \n",
    "    > conda install -c conda-forge ffmpeg\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f738061191e1fb55"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import basic packages\n",
    "import os\n",
    "import numpy as np\n",
    "import wget\n",
    "import sys\n",
    "import gdown\n",
    "import zipfile\n",
    "import librosa\n",
    "import pandas as pd\n",
    "# in the notebook, we only can use one GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c423c48088a2bb3b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Build the workspace and download the needed files\n",
    "\n",
    "def create_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "workspace = \"./workspace_ADS_v3\"\n",
    "dataset_path = os.path.join(workspace, \"mfg_robot\")\n",
    "checkpoint_path = os.path.join(workspace, \"ckpt\")\n",
    "mfg_raw_path = os.path.join(dataset_path, \"raw\")\n",
    "master_path = os.path.join(mfg_raw_path,\"MFG-master\")\n",
    "\n",
    "\n",
    "create_path(workspace)\n",
    "create_path(dataset_path)\n",
    "create_path(checkpoint_path)\n",
    "create_path(mfg_raw_path)\n",
    "create_path(master_path)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aed19ecc484e3fd9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Process Manufacturing Dataset – Resampling Audio Files\n",
    "meta_path = os.path.join(mfg_raw_path, 'MFG-master', 'meta', 'meta.csv')\n",
    "audio_path = os.path.join(mfg_raw_path, 'MFG-master', 'CurrentRotationTorque')\n",
    "resample_path = os.path.join(dataset_path, 'resample')\n",
    "savedata_path = os.path.join(dataset_path, 'mfg-data.npy')\n",
    "create_path(resample_path)\n",
    "\n",
    "\n",
    "# load the meta you generated earlier\n",
    "new_meta = pd.read_csv(r'C:\\Users\\Louis\\PycharmProjects\\HTS-Audio-Transformer\\workspace_ADS_v3\\mfg_robot\\raw\\MFG-master\\meta\\meta.csv')\n",
    "\n",
    "# overwrite the tutorial’s meta.csv so the script picks it up\n",
    "new_meta.to_csv(meta_path, index=False)\n",
    "\n",
    "meta = np.loadtxt(meta_path , delimiter=',', dtype='str', skiprows=1)\n",
    "audio_list = os.listdir(audio_path)\n",
    "\n",
    "print(\"-------------Resample MFG-------------\")\n",
    "for f in audio_list:\n",
    "    full_f = os.path.join(audio_path, f)\n",
    "    resample_f = os.path.join(resample_path, f)\n",
    "    if not os.path.exists(resample_f):\n",
    "        os.system('sox -V1 ' + full_f + ' -r 32000 ' + resample_f)\n",
    "print(\"-------------Resample Success-------------\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53019f402773ee3f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "meta = np.loadtxt(meta_path , delimiter=',', dtype='str', skiprows=1)\n",
    "print(f\"Loaded {len(meta)} samples from meta.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "449017a469ab6acf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"-------------Build Dataset-------------\")\n",
    "output_dict = [[] for _ in range(5)]\n",
    "for label in meta:\n",
    "    name = label[0]\n",
    "    fold = label[1]\n",
    "    target = label[2]\n",
    "    y, sr = librosa.load(os.path.join(resample_path, name), sr = None)\n",
    "    output_dict[int(fold) - 1].append(\n",
    "        {\n",
    "            \"name\": name,\n",
    "            \"target\": int(target),\n",
    "            \"waveform\": y\n",
    "        }\n",
    "    )\n",
    "    \n",
    "output_arr = np.array(output_dict, dtype=object)\n",
    "np.save(savedata_path, output_arr)\n",
    "print(\"-------------Success-------------\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67b747f78084d5a0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "full_dataset = np.load(savedata_path, allow_pickle=True)\n",
    "print([len(fold) for fold in full_dataset])   # should sum to 89, e.g. [18, 18, 18, 18, 17]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90fba165a8296de2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the model package\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import warnings\n",
    "\n",
    "from utils import create_folder, dump_config, process_idc\n",
    "import mfg_config as config\n",
    "from sed_model import SEDWrapper, Ensemble_SEDWrapper\n",
    "from data_generator import ESC_Dataset\n",
    "from model.htsat import HTSAT_Swin_Transformer\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20012d67adb8c549",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "class data_prep(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, eval_dataset, device_num):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.device_num = device_num\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_sampler = DistributedSampler(self.train_dataset, shuffle = False) if self.device_num > 1 else None\n",
    "        train_loader = DataLoader(\n",
    "            dataset = self.train_dataset,\n",
    "            num_workers = config.num_workers,\n",
    "            batch_size = config.batch_size // self.device_num,\n",
    "            shuffle = False,\n",
    "            sampler = train_sampler\n",
    "        )\n",
    "        \n",
    "        return train_loader\n",
    "    def val_dataloader(self):\n",
    "        eval_sampler = DistributedSampler(self.eval_dataset, shuffle = False) if self.device_num > 1 else None\n",
    "        eval_loader = DataLoader(\n",
    "            dataset = self.eval_dataset,\n",
    "            num_workers = config.num_workers,\n",
    "            batch_size = config.batch_size // self.device_num,\n",
    "            shuffle = False,\n",
    "            sampler = eval_sampler\n",
    "        )\n",
    "        return eval_loader\n",
    "    def test_dataloader(self):\n",
    "        test_sampler = DistributedSampler(self.eval_dataset, shuffle = False) if self.device_num > 1 else None\n",
    "        test_loader = DataLoader(\n",
    "            dataset = self.eval_dataset,\n",
    "            num_workers = config.num_workers,\n",
    "            batch_size = config.batch_size // self.device_num,\n",
    "            shuffle = False,\n",
    "            sampler = test_sampler\n",
    "        )\n",
    "        return test_loader\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0ff2f696c5a6f4e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set the workspace\n",
    "device_num = torch.cuda.device_count()\n",
    "print(\"device_num:\", device_num)\n",
    "print(\"each batch size:\", config.batch_size // device_num)\n",
    "\n",
    "full_dataset = np.load(os.path.join(config.dataset_path, \"mfg-data.npy\"), allow_pickle = True)\n",
    "\n",
    "# set exp folder\n",
    "exp_dir = os.path.join(config.workspace, \"results\", config.exp_name)\n",
    "checkpoint_dir = os.path.join(config.workspace, \"results\", config.exp_name, \"checkpoint\")\n",
    "if not config.debug:\n",
    "    create_folder(os.path.join(config.workspace, \"results\"))\n",
    "    create_folder(exp_dir)\n",
    "    create_folder(checkpoint_dir)\n",
    "    dump_config(config, os.path.join(exp_dir, config.exp_name), False)\n",
    "\n",
    "print(\"Using ESC Dataset in data_generator.py\")\n",
    "dataset = ESC_Dataset(\n",
    "    dataset = full_dataset,\n",
    "    config = config,\n",
    "    eval_mode = False\n",
    ")\n",
    "eval_dataset = ESC_Dataset(\n",
    "    dataset = full_dataset,\n",
    "    config = config,\n",
    "    eval_mode = True\n",
    ")\n",
    "\n",
    "audioset_data = data_prep(dataset, eval_dataset, device_num)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor = \"acc\",\n",
    "    filename='l-{epoch:d}-{acc:.3f}',\n",
    "    save_top_k = 20,\n",
    "    mode = \"max\"\n",
    ")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94d29a2763d873d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Set the Trainer\n",
    "trainer = pl.Trainer(\n",
    "    deterministic=False,\n",
    "    default_root_dir = checkpoint_dir,\n",
    "    gpus = device_num, \n",
    "    val_check_interval = 1.0,\n",
    "    max_epochs = config.max_epoch,\n",
    "    auto_lr_find = True,    \n",
    "    sync_batchnorm = True,\n",
    "    callbacks = [checkpoint_callback],\n",
    "    accelerator = \"ddp\" if device_num > 1 else None,\n",
    "    num_sanity_val_steps = 0,\n",
    "    resume_from_checkpoint = None, \n",
    "    replace_sampler_ddp = False,\n",
    "    log_every_n_steps=1, \n",
    "    gradient_clip_val=1.0\n",
    ")\n",
    "\n",
    "sed_model = HTSAT_Swin_Transformer(\n",
    "    spec_size=config.htsat_spec_size,\n",
    "    patch_size=config.htsat_patch_size,\n",
    "    in_chans=1,\n",
    "    num_classes=config.classes_num,\n",
    "    window_size=config.htsat_window_size,\n",
    "    config = config,\n",
    "    depths = config.htsat_depth,\n",
    "    embed_dim = config.htsat_dim,\n",
    "    patch_stride=config.htsat_stride,\n",
    "    num_heads=config.htsat_num_head\n",
    ")\n",
    "\n",
    "model = SEDWrapper(\n",
    "    sed_model = sed_model, \n",
    "    config = config,\n",
    "    dataset = dataset\n",
    ")\n",
    "\n",
    "if config.resume_checkpoint is not None:\n",
    "    print(\"Load Checkpoint from \", config.resume_checkpoint)\n",
    "    ckpt = torch.load(config.resume_checkpoint, map_location=\"cpu\")\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.head.weight\")\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.head.bias\")\n",
    "    # finetune on the esc and spv2 dataset\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.tscam_conv.weight\")\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.tscam_conv.bias\")\n",
    "    model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b38f7f9ec85653e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=[train_loader],\n",
    "    val_dataloaders=  [val_loader],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0b672f5f275ef1f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"Train samples:\", len(audioset_data.train_dataset))\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebf462a7e8f90ede",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# make absolutely sure both loaders use num_workers=0:\n",
    "train_loader = DataLoader(\n",
    "    audioset_data.train_dataset,\n",
    "    batch_size=config.batch_size // device_num,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"Train batches:\", len(train_loader))\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    audioset_data.eval_dataset,\n",
    "    batch_size=config.batch_size // device_num,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# now pass them explicitly:\n",
    "# trainer.fit(\n",
    "#     model,\n",
    "#     datamodule=audioset_data\n",
    "# )\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloader=train_loader,   # singular\n",
    "    val_dataloader= val_loader\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42deb54b34977029",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# infer the single data to check the result\n",
    "# get a model you saved\n",
    "model_path = r\"C:\\Users\\Louis\\PycharmProjects\\HTS-Audio-Transformer\\workspace_ADS_v3\\results\\exp_htsat_mfg\\checkpoint\\lightning_logs\\version_9\\checkpoints\\l-epoch=3-acc=1.000.ckpt\"\n",
    "\n",
    "# get the groundtruth\n",
    "meta = np.loadtxt(meta_path , delimiter=',', dtype='str', skiprows=1)\n",
    "gd = {}\n",
    "for label in meta:\n",
    "    name = label[0]\n",
    "    target = label[2]\n",
    "    gd[name] = target\n",
    "\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "# class Audio_Classification:\n",
    "#     def __init__(self, model_path, config):\n",
    "#         # Device\n",
    "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# \n",
    "#         # Build model\n",
    "#         self.sed_model = HTSAT_Swin_Transformer(\n",
    "#             spec_size=config.htsat_spec_size,\n",
    "#             patch_size=config.htsat_patch_size,\n",
    "#             in_chans=1,\n",
    "#             num_classes=config.classes_num,\n",
    "#             window_size=config.htsat_window_size,\n",
    "#             config=config,\n",
    "#             depths=config.htsat_depth,\n",
    "#             embed_dim=config.htsat_dim,\n",
    "#             patch_stride=config.htsat_stride,\n",
    "#             num_heads=config.htsat_num_head\n",
    "#         )\n",
    "# \n",
    "#         # Load checkpoint\n",
    "#         ckpt = torch.load(model_path, map_location='cpu')\n",
    "#         # strip Lightning prefixes if needed\n",
    "#         state_dict = {k.replace('sed_model.', ''): v for k, v in ckpt['state_dict'].items()}\n",
    "#         self.sed_model.load_state_dict(state_dict, strict=False)\n",
    "# \n",
    "#         # Move to device and eval mode\n",
    "#         self.sed_model.to(self.device)\n",
    "#         self.sed_model.eval()\n",
    "# \n",
    "#         # Fixed-length settings (must match training)\n",
    "#         self.SR         = 32000\n",
    "#         self.TARGET_SEC = 5\n",
    "#         self.fixed_len  = self.SR * self.TARGET_SEC\n",
    "# \n",
    "#     def predict(self, audiofile):\n",
    "#         # 1) Load & resample\n",
    "#         waveform, sr = librosa.load(audiofile, sr=self.SR)\n",
    "# \n",
    "#         # 2) Pad or truncate to fixed length\n",
    "#         waveform = librosa.util.fix_length(waveform, size=self.fixed_len)\n",
    "# \n",
    "#         # 3) To tensor & add batch + channel dims: [1,1,T]\n",
    "#         x = torch.from_numpy(waveform).float().to(self.device)\n",
    "#         x = x.unsqueeze(0).unsqueeze(0)\n",
    "# \n",
    "#         # 4) Forward pass\n",
    "#         with torch.no_grad():\n",
    "#             output_dict = self.sed_model(x, None, True)\n",
    "#             post = output_dict['clipwise_output'][0].cpu().numpy()\n",
    "#             pred_label = int(np.argmax(post))\n",
    "#             pred_prob  = float(np.max(post))\n",
    "# \n",
    "#         return pred_label, pred_prob\n",
    "\n",
    "class Audio_Classification:\n",
    "    def __init__(self, model_path, config):\n",
    "        # 0️⃣ Device selection\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # 1️⃣ Build the HTS‑AT model\n",
    "        self.sed_model = HTSAT_Swin_Transformer(\n",
    "            spec_size    = config.htsat_spec_size,\n",
    "            patch_size   = config.htsat_patch_size,\n",
    "            in_chans     = 1,\n",
    "            num_classes  = config.classes_num,\n",
    "            window_size  = config.htsat_window_size,\n",
    "            config       = config,\n",
    "            depths       = config.htsat_depth,\n",
    "            embed_dim    = config.htsat_dim,\n",
    "            patch_stride = config.htsat_stride,\n",
    "            num_heads    = config.htsat_num_head\n",
    "        )\n",
    "\n",
    "        # 2️⃣ Load checkpoint weights\n",
    "        ckpt = torch.load(model_path, map_location='cpu')\n",
    "        state_dict = {\n",
    "            k.replace('sed_model.', ''): v\n",
    "            for k, v in ckpt['state_dict'].items()\n",
    "        }\n",
    "        self.sed_model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        # 3️⃣ Disable STFT centering/padding to avoid the NotImplementedError\n",
    "        try:\n",
    "            self.sed_model.spectrogram_extractor.stft.center = False\n",
    "        except AttributeError:\n",
    "            # If your extractor structure differs, skip silently\n",
    "            pass\n",
    "\n",
    "        # 4️⃣ Move model to device and set eval mode\n",
    "        self.sed_model.to(self.device)\n",
    "        self.sed_model.eval()\n",
    "\n",
    "        # 5️⃣ Fixed‑length audio settings (must match training)\n",
    "        self.SR         = 32000\n",
    "        self.TARGET_SEC = 5\n",
    "        self.fixed_len  = self.SR * self.TARGET_SEC\n",
    "\n",
    "    def predict(self, audiofile: str) -> (int, float):\n",
    "        \"\"\"\n",
    "        Load an audio file, pad/truncate to fixed length,\n",
    "        run it through the model, and return (label, probability).\n",
    "        \"\"\"\n",
    "        # a) Load & resample\n",
    "        waveform, sr = librosa.load(audiofile, sr=self.SR)\n",
    "\n",
    "        # b) Pad or truncate to exactly fixed_len\n",
    "        waveform = librosa.util.fix_length(waveform, size=self.fixed_len)\n",
    "\n",
    "        # c) To tensor & add batch dimension -> [1, T]\n",
    "        x = torch.from_numpy(waveform).float().to(self.device).unsqueeze(0)\n",
    "\n",
    "        # d) Forward pass\n",
    "        with torch.no_grad():\n",
    "            output_dict = self.sed_model(x, None, True)\n",
    "            post = output_dict['clipwise_output'][0].cpu().numpy()\n",
    "            pred_label = int(np.argmax(post))\n",
    "            pred_prob  = float(np.max(post))\n",
    "\n",
    "        return pred_label, pred_prob"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "944a4ba0fbcd5e0f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Inference\n",
    "Audiocls = Audio_Classification(model_path, config)\n",
    "\n",
    "# pick any audio you like in the ESC-50 testing set (cross-validation)\n",
    "pred_label, pred_prob = Audiocls.predict(r\"C:\\Users\\Louis\\PycharmProjects\\HTS-Audio-Transformer\\workspace_ADS_v3\\mfg_robot\\raw\\MFG-master\\CurrentRotationTorque\\cycle_026.wav\")\n",
    "\n",
    "print('Audiocls predict output: ', pred_label, pred_prob, gd[\"cycle_026.wav\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad190377d5333ddd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "clf = Audio_Classification(model_path, config)\n",
    "label, prob = clf.predict(audio_path)\n",
    "print(\"Pred:\", label, \"prob:\", prob, \"GT:\", true_label)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff85b939cced6ab4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1) Instantiate classifier\n",
    "clf = Audio_Classification(model_path, config)\n",
    "\n",
    "# 2) Point to a specific .wav file\n",
    "test_file = os.path.join(\n",
    "    r\"C:\\Users\\Louis\\PycharmProjects\\HTS-Audio-Transformer\",\n",
    "    \"workspace_ADS_v3\",\n",
    "    \"mfg_robot\",\n",
    "    \"raw\",\n",
    "    \"MFG-master\",\n",
    "    \"CurrentRotationTorque\",\n",
    "    \"cycle_026.wav\"\n",
    ")\n",
    "\n",
    "# 3) Run prediction\n",
    "pred_label, pred_prob = clf.predict(test_file)\n",
    "\n",
    "# 4) Compare to ground truth\n",
    "true_label = int(gd[\"cycle_062.wav\"])\n",
    "print(f\"Predicted: {pred_label} (p={pred_prob:.3f}),  True: {true_label}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38c9b0ec40a09bec",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1d435129cc9fa26f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a306bd254d41339b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
