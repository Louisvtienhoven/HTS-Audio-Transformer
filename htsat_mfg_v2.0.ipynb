{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial on training a HTS-AT model for audio classification on the ESC-50 Dataset\n",
    "\n",
    "Referece: \n",
    "\n",
    "[HTS-AT: A Hierarchical Token-Semantic Audio Transformer for Sound Classification and Detection, ICASSP 2022](https://arxiv.org/abs/2202.00874)\n",
    "\n",
    "Following the HTS-AT's paper, in this tutorial, we would show how to use the HST-AT in the training of the ESC-50 Dataset.\n",
    "\n",
    "The [ESC-50 dataset](https://github.com/karolpiczak/ESC-50) is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification. The dataset consists of 5-second-long recordings organized into 50 semantical classes (with 40 examples per class) loosely arranged into 5 major categories\n",
    "\n",
    "Before running this tutorial, please make sure that you install the below packages by following steps:\n",
    "\n",
    "1. download [the codebase](https://github.com/RetroCirce/HTS-Audio-Transformer), and put this tutorial notebook inside the codebase folder.\n",
    "\n",
    "2. In the github code folder:\n",
    "\n",
    "    > pip install -r requirements.txt\n",
    "\n",
    "3. We do not include the installation of PyTorch in the requirment, since different machines require different vereions of CUDA and Toolkits. So make sure you install the PyTorch from [the official guidance](https://pytorch.org/).\n",
    "\n",
    "4. Install the 'SOX' and the 'ffmpeg', we recommend that you run this code in Linux inside the Conda environment. In that, you can install them by:\n",
    "\n",
    "    > sudo apt install sox\n",
    "    \n",
    "    > conda install -c conda-forge ffmpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T15:32:47.658409200Z",
     "start_time": "2025-04-17T15:32:47.641679300Z"
    }
   },
   "outputs": [],
   "source": [
    "# import basic packages\n",
    "import os\n",
    "import numpy as np\n",
    "import wget\n",
    "import sys\n",
    "import gdown\n",
    "import zipfile\n",
    "import librosa\n",
    "# in the notebook, we only can use one GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T15:32:51.550371600Z",
     "start_time": "2025-04-17T15:32:51.530446200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the workspace and download the needed files\n",
    "\n",
    "def create_path(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "workspace = \"./workspace_ADS_v3\"\n",
    "dataset_path = os.path.join(workspace, \"mfg_robot\")\n",
    "checkpoint_path = os.path.join(workspace, \"ckpt\")\n",
    "mfg_raw_path = os.path.join(dataset_path, \"raw\")\n",
    "master_path = os.path.join(mfg_raw_path,\"MFG-master\")\n",
    "\n",
    "\n",
    "create_path(workspace)\n",
    "create_path(dataset_path)\n",
    "create_path(checkpoint_path)\n",
    "create_path(mfg_raw_path)\n",
    "create_path(master_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Resample MFG-------------\n",
      "-------------Resample Success-------------\n"
     ]
    }
   ],
   "source": [
    "# Process Manufacturing Dataset – Resampling Audio Files\n",
    "\n",
    "audio_path = os.path.join(mfg_raw_path, 'MFG-master', 'CurrentRotationTorque')\n",
    "resample_path = os.path.join(dataset_path, 'resample')\n",
    "savedata_path = os.path.join(dataset_path, 'mfg-data.npy')\n",
    "create_path(resample_path)\n",
    "\n",
    "audio_list = os.listdir(audio_path)\n",
    "\n",
    "print(\"-------------Resample MFG-------------\")\n",
    "for f in audio_list:\n",
    "    full_f = os.path.join(audio_path, f)\n",
    "    resample_f = os.path.join(resample_path, f)\n",
    "    if not os.path.exists(resample_f):\n",
    "        os.system('sox -V1 ' + full_f + ' -r 32000 ' + resample_f)\n",
    "print(\"-------------Resample Success-------------\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-17T15:32:56.511784700Z",
     "start_time": "2025-04-17T15:32:56.482347500Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "./workspace_ADS_v3\\mfg_robot\\raw\\MFG-master\\meta\\mfg.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Paths\u001B[39;00m\n\u001B[0;32m      6\u001B[0m meta_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(mfg_raw_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMFG-master\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mmeta\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mmfg.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m# Adjust this path if needed\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m meta \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloadtxt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmeta_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelimiter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m,\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mstr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskiprows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-------------Build Dataset-------------\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     10\u001B[0m output_dict \u001B[38;5;241m=\u001B[39m [[] \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m5\u001B[39m)]  \u001B[38;5;66;03m# Assuming 5 folds, still okay if we only use fold 1\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\numpy\\lib\\npyio.py:1042\u001B[0m, in \u001B[0;36mloadtxt\u001B[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001B[0m\n\u001B[0;32m   1040\u001B[0m     fname \u001B[38;5;241m=\u001B[39m os_fspath(fname)\n\u001B[0;32m   1041\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _is_string_like(fname):\n\u001B[1;32m-> 1042\u001B[0m     fh \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_datasource\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1043\u001B[0m     fencoding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(fh, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlatin1\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m   1044\u001B[0m     line_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(fh)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(path, mode, destpath, encoding, newline)\u001B[0m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;124;03mOpen `path` with `mode` and return the file object.\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    189\u001B[0m \n\u001B[0;32m    190\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    192\u001B[0m ds \u001B[38;5;241m=\u001B[39m DataSource(destpath)\n\u001B[1;32m--> 193\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnewline\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\numpy\\lib\\_datasource.py:532\u001B[0m, in \u001B[0;36mDataSource.open\u001B[1;34m(self, path, mode, encoding, newline)\u001B[0m\n\u001B[0;32m    529\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _file_openers[ext](found, mode\u001B[38;5;241m=\u001B[39mmode,\n\u001B[0;32m    530\u001B[0m                               encoding\u001B[38;5;241m=\u001B[39mencoding, newline\u001B[38;5;241m=\u001B[39mnewline)\n\u001B[0;32m    531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 532\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: ./workspace_ADS_v3\\mfg_robot\\raw\\MFG-master\\meta\\mfg.csv not found."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Paths\n",
    "meta_path = os.path.join(mfg_raw_path, 'MFG-master\\meta\\mfg.csv')  # Adjust this path if needed\n",
    "meta = np.loadtxt(meta_path, delimiter=',', dtype='str', skiprows=1)\n",
    "\n",
    "print(\"-------------Build Dataset-------------\")\n",
    "output_dict = [[] for _ in range(5)]  # Assuming 5 folds, still okay if we only use fold 1\n",
    "\n",
    "for label in meta:\n",
    "    name = label[0]\n",
    "    fold = int(label[1])\n",
    "    target = int(label[2])\n",
    "    \n",
    "    #y, sr = librosa.load(os.path.join(resample_path, name), sr=None)\n",
    "    \n",
    "    # Preserve the orinal multi-channel structure for the sensor data\n",
    "    y, sr = librosa.load(os.path.join(resample_path, name), sr=None, mono=False)\n",
    "\n",
    "    output_dict[fold - 1].append({\n",
    "        \"name\": name,\n",
    "        \"target\": target,\n",
    "        \"waveform\": y\n",
    "    })\n",
    "\n",
    "np.save(savedata_path, np.array(output_dict, dtype=object))\n",
    "\n",
    "print(\"-------------Build Dataset Success-------------\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-17T15:03:57.846257300Z",
     "start_time": "2025-04-17T15:03:57.561825400Z"
    }
   },
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-17T15:03:57.840830900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the model package\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import warnings\n",
    "\n",
    "from utils import create_folder, dump_config, process_idc\n",
    "import mfg_config as config\n",
    "from sed_model import SEDWrapper, Ensemble_SEDWrapper\n",
    "from data_generator import ESC_Dataset\n",
    "from model.htsat_mfg_v_2_0 import HTSAT_Swin_Transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "# New data preparation class\n",
    "class data_prep(pl.LightningDataModule):\n",
    "    def __init__(self, dataset, config, device_num):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset  # Store only a reference\n",
    "        self.config = config\n",
    "        self.device_num = device_num\n",
    "        self.train_dataset = None  # Placeholder, will be initialized later\n",
    "        self.eval_dataset = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"This method is called inside Lightning, and it ensures datasets are created properly.\"\"\"\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = MFG_Dataset(\n",
    "                dataset=self.dataset,\n",
    "                config=self.config,\n",
    "                eval_mode=False\n",
    "            )\n",
    "            self.eval_dataset = MFG_Dataset(\n",
    "                dataset=self.dataset,\n",
    "                config=self.config,\n",
    "                eval_mode=True\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_sampler = DistributedSampler(self.train_dataset, shuffle=False) if self.device_num > 1 else None\n",
    "        return DataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            num_workers=self.config.num_workers,\n",
    "            batch_size=self.config.batch_size // max(1, self.device_num),\n",
    "            shuffle=False,\n",
    "            sampler=train_sampler\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        eval_sampler = DistributedSampler(self.eval_dataset, shuffle=False) if self.device_num > 1 else None\n",
    "        return DataLoader(\n",
    "            dataset=self.eval_dataset,\n",
    "            num_workers=self.config.num_workers,\n",
    "            batch_size=self.config.batch_size // max(1, self.device_num),\n",
    "            shuffle=False,\n",
    "            sampler=eval_sampler\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_sampler = DistributedSampler(self.eval_dataset, shuffle=False) if self.device_num > 1 else None\n",
    "        return DataLoader(\n",
    "            dataset=self.eval_dataset,\n",
    "            num_workers=self.config.num_workers,\n",
    "            batch_size=self.config.batch_size // max(1, self.device_num),\n",
    "            shuffle=False,\n",
    "            sampler=test_sampler\n",
    "        )\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        \"\"\"Removes unpicklable attributes before multiprocessing starts\"\"\"\n",
    "        for attr in [\"trainer\", \"prepare_data\", \"setup\", \"teardown\"]:\n",
    "            if hasattr(self, attr):\n",
    "                delattr(self, attr)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-17T15:03:57.844156Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T15:03:57.851444600Z",
     "start_time": "2025-04-17T15:03:57.847255600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the workspace\n",
    "device_num = torch.cuda.device_count()\n",
    "print(\"device_num:\", device_num)\n",
    "print(\"each batch size:\", config.batch_size // device_num)\n",
    "\n",
    "full_dataset = np.load(os.path.join(config.dataset_path, \"mfg-data.npy\"), allow_pickle = True)\n",
    "\n",
    "# set exp folder\n",
    "exp_dir = os.path.join(config.workspace, \"results\", config.exp_name)\n",
    "checkpoint_dir = os.path.join(config.workspace, \"results\", config.exp_name, \"checkpoint\")\n",
    "if not config.debug:\n",
    "    create_folder(os.path.join(config.workspace, \"results\"))\n",
    "    create_folder(exp_dir)\n",
    "    create_folder(checkpoint_dir)\n",
    "    dump_config(config, os.path.join(exp_dir, config.exp_name), False)\n",
    "\n",
    "print(\"Using ESC Dataset\")\n",
    "dataset = ESC_Dataset(\n",
    "    dataset = full_dataset,\n",
    "    config = config,\n",
    "    eval_mode = False\n",
    ")\n",
    "eval_dataset = ESC_Dataset(\n",
    "    dataset = full_dataset,\n",
    "    config = config,\n",
    "    eval_mode = True\n",
    ")\n",
    "\n",
    "audioset_data = data_prep(dataset, eval_dataset, device_num)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor = \"acc\",\n",
    "    filename='l-{epoch:d}-{acc:.3f}',\n",
    "    save_top_k = 20,\n",
    "    mode = \"max\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T15:03:57.889350800Z",
     "start_time": "2025-04-17T15:03:57.852447500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the Trainer\n",
    "trainer = pl.Trainer(\n",
    "    deterministic=False,\n",
    "    default_root_dir=checkpoint_dir,\n",
    "    gpus=device_num, \n",
    "    val_check_interval=1.0,\n",
    "    max_epochs=config.max_epoch,\n",
    "    auto_lr_find=True,    \n",
    "    sync_batchnorm=True,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    accelerator=\"ddp\" if device_num > 1 else None,\n",
    "    num_sanity_val_steps=0,\n",
    "    resume_from_checkpoint=None, \n",
    "    replace_sampler_ddp=False,\n",
    "    gradient_clip_val=1.0\n",
    ")\n",
    "\n",
    "# Create the HTSAT model with updated channel input (e.g., 3 or 6 channels)\n",
    "sed_model = HTSAT_Swin_Transformer(\n",
    "    spec_size=config.htsat_spec_size,\n",
    "    patch_size=config.htsat_patch_size,\n",
    "    in_chans=3,  # Change to 3 or 6 depending on your sensor data channels\n",
    "    num_classes=config.classes_num,\n",
    "    window_size=config.htsat_window_size,\n",
    "    config=config,\n",
    "    depths=config.htsat_depth,\n",
    "    embed_dim=config.htsat_dim,\n",
    "    patch_stride=config.htsat_stride,\n",
    "    num_heads=config.htsat_num_head\n",
    ")\n",
    "\n",
    "model = SEDWrapper(\n",
    "    sed_model=sed_model, \n",
    "    config=config,\n",
    "    dataset=dataset\n",
    ")\n",
    "\n",
    "if config.resume_checkpoint is not None:\n",
    "    print(\"Load Checkpoint from \", config.resume_checkpoint)\n",
    "    ckpt = torch.load(config.resume_checkpoint, map_location=\"cpu\")\n",
    "    \n",
    "    key = \"sed_model.patch_embed.proj.weight\"\n",
    "    if key in ckpt[\"state_dict\"]:\n",
    "        weight = ckpt[\"state_dict\"][key]\n",
    "        # Adapt the patch embedding weights to match the current in_chans setting\n",
    "        if weight.shape[1] != sed_model.in_chans:\n",
    "            # Assume the checkpoint was trained with a single channel (in_chans==1)\n",
    "            if weight.shape[1] == 1:\n",
    "                adapted_weight = weight.repeat(1, sed_model.in_chans, 1, 1) / sed_model.in_chans\n",
    "                ckpt[\"state_dict\"][key] = adapted_weight\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected number of channels in checkpoint weight: {}\".format(weight.shape[1]))\n",
    "    \n",
    "    # Remove keys that might conflict with the current model architecture\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.head.weight\", None)\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.head.bias\", None)\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.tscam_conv.weight\", None)\n",
    "    ckpt[\"state_dict\"].pop(\"sed_model.tscam_conv.bias\", None)\n",
    "    \n",
    "    model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the saved dataset from the npy file\n",
    "full_dataset = np.load(os.path.join(config.dataset_path, \"mfg-data.npy\"), allow_pickle=True)\n",
    "\n",
    "print(\"Before conversion:\")\n",
    "for fold in full_dataset:\n",
    "    for sample in fold:\n",
    "        print(sample[\"name\"], sample[\"waveform\"].shape)\n",
    "\n",
    "def convert_mono_to_three(waveform):\n",
    "    # If waveform is 1D (mono), convert it to shape (3, n_samples)\n",
    "    if waveform.ndim == 1:\n",
    "        return np.tile(waveform, (3, 1))\n",
    "    # If waveform is 2D and has only one channel, repeat along the channel axis\n",
    "    elif waveform.ndim == 2 and waveform.shape[0] == 1:\n",
    "        return np.repeat(waveform, 3, axis=0)\n",
    "    else:\n",
    "        return waveform\n",
    "\n",
    "# Convert all waveforms in the dataset\n",
    "for fold in full_dataset:\n",
    "    for sample in fold:\n",
    "        sample[\"waveform\"] = convert_mono_to_three(sample[\"waveform\"])\n",
    "\n",
    "print(\"\\nAfter conversion:\")\n",
    "for fold in full_dataset:\n",
    "    for sample in fold:\n",
    "        print(sample[\"name\"], sample[\"waveform\"].shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-17T15:03:57.855871400Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved dataset from the npy file\n",
    "full_dataset = np.load(os.path.join(config.dataset_path, \"mfg-data.npy\"), allow_pickle=True)\n",
    "\n",
    "print(\"Before conversion:\")\n",
    "for fold in full_dataset:\n",
    "    for sample in fold:\n",
    "        shape = sample[\"waveform\"].shape\n",
    "        # If waveform is 1D, we assume it's mono (1 channel). Otherwise, the first dimension indicates channels.\n",
    "        num_channels = 1 if sample[\"waveform\"].ndim == 1 else sample[\"waveform\"].shape[0]\n",
    "        print(f\"{sample['name']}: shape {shape}, channels: {num_channels}\")\n",
    "\n",
    "def convert_mono_to_three(waveform):\n",
    "    # If waveform is 1D (mono), convert it to shape (3, n_samples)\n",
    "    if waveform.ndim == 1:\n",
    "        return np.tile(waveform, (3, 1))\n",
    "    # If waveform is 2D and has only one channel, repeat along the channel axis\n",
    "    elif waveform.ndim == 2 and waveform.shape[0] == 1:\n",
    "        return np.repeat(waveform, 3, axis=0)\n",
    "    else:\n",
    "        return waveform\n",
    "\n",
    "# Convert all waveforms in the dataset from mono to three channels (if needed)\n",
    "for fold in full_dataset:\n",
    "    for sample in fold:\n",
    "        sample[\"waveform\"] = convert_mono_to_three(sample[\"waveform\"])\n",
    "\n",
    "print(\"\\nAfter conversion:\")\n",
    "for fold in full_dataset:\n",
    "    for sample in fold:\n",
    "        shape = sample[\"waveform\"].shape\n",
    "        num_channels = 1 if sample[\"waveform\"].ndim == 1 else sample[\"waveform\"].shape[0]\n",
    "        print(f\"{sample['name']}: shape {shape}, channels: {num_channels}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-17T15:03:57.858134700Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "def dynamic_pad_collate(batch):\n",
    "    # Convert each sample's waveform to a tensor (if needed)\n",
    "    for sample in batch:\n",
    "        if not isinstance(sample[\"waveform\"], torch.Tensor):\n",
    "            sample[\"waveform\"] = torch.tensor(sample[\"waveform\"], dtype=torch.float)\n",
    "        # Ensure the waveform has at least two dimensions: [channels, time]\n",
    "        if sample[\"waveform\"].dim() == 1:\n",
    "            sample[\"waveform\"] = sample[\"waveform\"].unsqueeze(0)\n",
    "    # Determine the maximum time dimension (axis=1) among samples\n",
    "    max_length = max(sample[\"waveform\"].shape[1] for sample in batch)\n",
    "    padded_waveforms, targets, names, lengths = [], [], [], []\n",
    "    for sample in batch:\n",
    "        wav = sample[\"waveform\"]  # expected shape: [channels, time]\n",
    "        current_length = wav.shape[1]\n",
    "        if current_length < max_length:\n",
    "            pad_amount = int(max_length - current_length)\n",
    "            # Pad along the time dimension\n",
    "            wav = F.pad(wav, (0, pad_amount), mode=\"constant\", value=0)\n",
    "        padded_waveforms.append(wav)\n",
    "        targets.append(sample[\"target\"])\n",
    "        names.append(sample[\"audio_name\"])\n",
    "        lengths.append(current_length)\n",
    "    return {\n",
    "        \"waveform\": torch.stack(padded_waveforms),  # shape: [B, channels, max_length]\n",
    "        \"target\": torch.tensor(targets),\n",
    "        \"audio_name\": names,\n",
    "        \"real_len\": torch.tensor(lengths)\n",
    "    }\n",
    "\n",
    "# Instantiate DataModule and ensure setup()\n",
    "audioset_data = data_prep(full_dataset, config, device_num)\n",
    "audioset_data.setup(\"fit\")\n",
    "if hasattr(audioset_data, \"trainer\"):\n",
    "    del audioset_data.trainer\n",
    "\n",
    "# Override DataLoaders to use dynamic_pad_collate\n",
    "audioset_data.train_dataloader = lambda: DataLoader(\n",
    "    audioset_data.train_dataset,\n",
    "    batch_size=config.batch_size // max(1, device_num),\n",
    "    sampler=DistributedSampler(audioset_data.train_dataset) if device_num > 1 else None,\n",
    "    num_workers=0,  # For debugging; increase as needed later\n",
    "    collate_fn=dynamic_pad_collate\n",
    ")\n",
    "audioset_data.val_dataloader = lambda: DataLoader(\n",
    "    audioset_data.eval_dataset,\n",
    "    batch_size=config.batch_size // max(1, device_num),\n",
    "    sampler=DistributedSampler(audioset_data.eval_dataset) if device_num > 1 else None,\n",
    "    num_workers=0,\n",
    "    collate_fn=dynamic_pad_collate\n",
    ")\n",
    "\n",
    "# Build model with 3-channel input (since your data now has 3 channels)\n",
    "sed_model = HTSAT_Swin_Transformer(\n",
    "    spec_size=config.htsat_spec_size,\n",
    "    patch_size=config.htsat_patch_size,\n",
    "    in_chans=3,  # Use 3-channel input\n",
    "    num_classes=config.classes_num,\n",
    "    window_size=config.htsat_window_size,\n",
    "    config=config,\n",
    "    depths=config.htsat_depth,\n",
    "    embed_dim=config.htsat_dim,\n",
    "    patch_stride=config.htsat_stride,\n",
    "    num_heads=config.htsat_num_head\n",
    ")\n",
    "model = SEDWrapper(sed_model=sed_model, config=config, dataset=audioset_data.train_dataset)\n",
    "\n",
    "# Trainer setup\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=checkpoint_dir,\n",
    "    gpus=device_num,\n",
    "    max_epochs=config.max_epoch,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    accelerator=\"ddp\" if device_num > 1 else None,\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "\n",
    "print(\"Starting training…\")\n",
    "trainer.fit(model, audioset_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-17T15:03:57.860323800Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Cell to check number of input channels\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "# Path to the resampled audio files\n",
    "resample_path = os.path.join(dataset_path, 'resample')\n",
    "\n",
    "# List all audio files in the resampled folder\n",
    "audio_files = os.listdir(resample_path)\n",
    "\n",
    "print(\"Checking number of channels for each audio file:\")\n",
    "for f in audio_files:\n",
    "    full_path = os.path.join(resample_path, f)\n",
    "    # Load the file with mono=False to keep multi-channel data if present\n",
    "    y, sr = librosa.load(full_path, sr=None, mono=False)\n",
    "    # If y is 1D, then it's mono. Otherwise, its first dimension is the channel count.\n",
    "    num_channels = 1 if y.ndim == 1 else y.shape[0]\n",
    "    print(f\"File: {f}, Shape: {y.shape}, Channels: {num_channels}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-17T15:03:57.861396Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-17T15:03:57.864978700Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-17T15:03:57.867281400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Let us Check the Result\n",
    "\n",
    "Find the path of your saved checkpoint and paste it in the below variable.\n",
    "Then you are able to follow the below code for checking the prediction result of any sample you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-17T15:03:57.869369500Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "./workspace_ADS_v3\\mfg_robot\\raw\\MFG-master\\meta\\mfg.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m model_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mLouis\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mPycharmProjects\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mHTS_AT-main\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mworkspace_ADS_v2\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mresults\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mexp_htsat_esc_50\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mcheckpoint\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mlightning_logs\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mversion_0\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mcheckpoints\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ml-epoch=10-acc=1.000.ckpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# get the groundtruth\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m meta \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloadtxt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmeta_path\u001B[49m\u001B[43m \u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelimiter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m,\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mstr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskiprows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m gd \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m label \u001B[38;5;129;01min\u001B[39;00m meta:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\numpy\\lib\\npyio.py:1042\u001B[0m, in \u001B[0;36mloadtxt\u001B[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001B[0m\n\u001B[0;32m   1040\u001B[0m     fname \u001B[38;5;241m=\u001B[39m os_fspath(fname)\n\u001B[0;32m   1041\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _is_string_like(fname):\n\u001B[1;32m-> 1042\u001B[0m     fh \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_datasource\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1043\u001B[0m     fencoding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(fh, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlatin1\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m   1044\u001B[0m     line_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(fh)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(path, mode, destpath, encoding, newline)\u001B[0m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;124;03mOpen `path` with `mode` and return the file object.\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    189\u001B[0m \n\u001B[0;32m    190\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    192\u001B[0m ds \u001B[38;5;241m=\u001B[39m DataSource(destpath)\n\u001B[1;32m--> 193\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnewline\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HTSAT_env\\lib\\site-packages\\numpy\\lib\\_datasource.py:532\u001B[0m, in \u001B[0;36mDataSource.open\u001B[1;34m(self, path, mode, encoding, newline)\u001B[0m\n\u001B[0;32m    529\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _file_openers[ext](found, mode\u001B[38;5;241m=\u001B[39mmode,\n\u001B[0;32m    530\u001B[0m                               encoding\u001B[38;5;241m=\u001B[39mencoding, newline\u001B[38;5;241m=\u001B[39mnewline)\n\u001B[0;32m    531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 532\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: ./workspace_ADS_v3\\mfg_robot\\raw\\MFG-master\\meta\\mfg.csv not found."
     ]
    }
   ],
   "source": [
    "# infer the single data to check the result\n",
    "# get a model you saved\n",
    "model_path = r\"C:\\Users\\Louis\\PycharmProjects\\HTS_AT-main\\workspace_ADS_v2\\results\\exp_htsat_esc_50\\checkpoint\\lightning_logs\\version_0\\checkpoints\\l-epoch=10-acc=1.000.ckpt\"\n",
    "\n",
    "# get the groundtruth\n",
    "meta = np.loadtxt(meta_path , delimiter=',', dtype='str', skiprows=1)\n",
    "gd = {}\n",
    "for label in meta:\n",
    "    name = label[0]\n",
    "    target = label[2]\n",
    "    gd[name] = target\n",
    "\n",
    "class Audio_Classification:\n",
    "    def __init__(self, model_path, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device('cuda')\n",
    "        self.sed_model = HTSAT_Swin_Transformer(\n",
    "            spec_size=config.htsat_spec_size,\n",
    "            patch_size=config.htsat_patch_size,\n",
    "            in_chans=1,\n",
    "            num_classes=config.classes_num,\n",
    "            window_size=config.htsat_window_size,\n",
    "            config = config,\n",
    "            depths = config.htsat_depth,\n",
    "            embed_dim = config.htsat_dim,\n",
    "            patch_stride=config.htsat_stride,\n",
    "            num_heads=config.htsat_num_head\n",
    "        )\n",
    "        ckpt = torch.load(model_path, map_location=\"cpu\")\n",
    "        temp_ckpt = {}\n",
    "        for key in ckpt[\"state_dict\"]:\n",
    "            temp_ckpt[key[10:]] = ckpt['state_dict'][key]\n",
    "        self.sed_model.load_state_dict(temp_ckpt)\n",
    "        self.sed_model.to(self.device)\n",
    "        self.sed_model.eval()\n",
    "\n",
    "\n",
    "    def predict(self, audiofile):\n",
    "\n",
    "        if audiofile:\n",
    "            waveform, sr = librosa.load(audiofile, sr=32000)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x = torch.from_numpy(waveform).float().to(self.device)\n",
    "                output_dict = self.sed_model(x[None, :], None, True)\n",
    "                pred = output_dict['clipwise_output']\n",
    "                pred_post = pred[0].detach().cpu().numpy()\n",
    "                pred_label = np.argmax(pred_post)\n",
    "                pred_prob = np.max(pred_post)\n",
    "            return pred_label, pred_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T15:03:57.972175700Z",
     "start_time": "2025-04-17T15:03:57.965624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inference\n",
    "Audiocls = Audio_Classification(model_path, config)\n",
    "\n",
    "# pick any audio you like in the ESC-50 testing set (cross-validation)\n",
    "pred_label, pred_prob = Audiocls.predict(r\"C:\\Users\\Louis\\PycharmProjects\\HTS_AT-main\\workspace_ADS_v2\\mfg_robot\\raw\\MFG-master\\audio\\rivet6.wav\")\n",
    "\n",
    "print('Audiocls predict output: ', pred_label, pred_prob, gd[\"rivet6.wav\"])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-17T15:03:57.967666200Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "htsat_env",
   "language": "python",
   "display_name": "Python (HTSAT_env)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb1a0df39641c41734bdd2d42699ec57167c4cf18fd061cdef52c16cce6262af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
